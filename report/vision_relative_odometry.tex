\documentclass[bare_jrnl_transmag]{subfiles}
\begin{document}

\subsection{Vision Relative Odometry Implementation}

Visual odometry involves extracting information from a series of images in order to estimate the pose and position of a camera. The dataset provides 30 frames per second stereo images allowing for 3D localization. 

Image features are first extracted from left and right pairs of images at a given time step. The Scale-Invariant Feature Transform \(SIFT\) feature extraction algorithm is used. The features of the two images are then matched to ensure the same points in both are compared. The Fast Library for Approximate Nearest Neighbors \(FLANN\) matcher is used to create feature matches between the two images.  Random sample consensus \(RANSAC\) is then used to filter these matches, only preserving the highest quality matches.  These steps are repeated for the previous time step, and the filtering is again applied, this time between the left images of the previous and current time step. This ensures that the two point clouds generated contain the same points such that the transformation between them can be found.

With the same features identified in both the left and right images, a 3D point cloud is generated by using triangulation to find the location of the points in 3D space. To do this the image is first undistorted to match a pinhole camera model. This is done by using the intrinsic transformation matrix provided in the calibration data. The point clouds at sequential time steps are then generated using triangulation, producing two point clouds with ideally the same points. To find the transformation between these two point clouds, outlier points, such as points behind the camera, are first removed. A custom iterative method is then used to find the transformation. A first guess transformation is applied to the previous time step's point cloud known as the source. The distances between the points in the source are now compared to the current time step's \(destination\) point cloud. Weights for each point are generated that are inversely proportional to the distances. The centroid of the point cloud is then found using the weights, and the centroids are aligned. The weighted covariance is found and PCA is used to obtain the principal components of the covariance. These principal axes are then aligned, and the rotation required to do this is saved as the rotation matrix. This rotation could then be applied to the source point cloud, and the translation between them is found through simple subtraction. This transformation is then fed back to the start of the process, and is repeated until the transformation matrix converges.

The system returns a relative transformation vector, t, and rotation matrix R. The relative rotation is not passed to the Kalman filter because the Madgwick filter estimation was found to be a more reliable pose reference. The drone's previous pose estimate from the Kalman filter is used to convert the relative translation into the world frame before using it in the update step.

\end{document}